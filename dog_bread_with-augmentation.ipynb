{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2291271f",
   "metadata": {},
   "source": [
    "### Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7630afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "# define function to load train, test, and validation datasets\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    dog_files = np.array(data['filenames'])\n",
    "    dog_targets = np_utils.to_categorical(np.array(data['target']), 133)\n",
    "    return dog_files, dog_targets\n",
    "\n",
    "# load train, test, and validation datasets\n",
    "train_files, train_targets = load_dataset('dogImages/train')\n",
    "valid_files, valid_targets = load_dataset('dogImages/valid')\n",
    "test_files, test_targets = load_dataset('dogImages/test')\n",
    "\n",
    "# load list of dog names\n",
    "# dog_names = [item[20:-1] for item in sorted(glob(\"../../../data/dog_images/train/*/\"))]\n",
    "dog_names = [item[20:-1] for item in sorted(glob('dogImages/train/*/'))]\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total dog categories.' % len(dog_names))\n",
    "print('There are %s total dog images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('There are %d training dog images.' % len(train_files))\n",
    "print('There are %d validation dog images.' % len(valid_files))\n",
    "print('There are %d test dog images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "899dbf00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13233 total human images.\n"
     ]
    }
   ],
   "source": [
    "#Import human files\n",
    "import random\n",
    "random.seed(8675309)\n",
    "\n",
    "# load filenames in shuffled human dataset\n",
    "# human_files = np.array(glob(\"../../../data/lfw/*/*\"))\n",
    "human_files = np.array(glob(\"data/lfw/*/*\"))\n",
    "random.shuffle(human_files)\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total human images.' % len(human_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "957808e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a122368c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 6680/6680 [00:52<00:00, 127.02it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 835/835 [00:20<00:00, 40.43it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 836/836 [00:20<00:00, 41.59it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# pre-process the data for Keras\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e87b7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Dropout, Flatten, Dense, Lambda\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fd5f01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6680 images belonging to 133 classes.\n",
      "Found 835 images belonging to 133 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen =ImageDataGenerator(\n",
    "            rescale = 1./255,\n",
    "            rotation_range=40,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            shear_range=0.2,\n",
    "            zoom_range=0.2,\n",
    "            horizontal_flip=True,\n",
    "            fill_mode='nearest')\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# this is a generator that will read pictures found in\n",
    "# subfolers of 'data/train', and indefinitely generate\n",
    "# batches of augmented image data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'dogImages/train',  # this is the target directory\n",
    "        target_size=(228, 228),  # all images will be resized to 150x150\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')  # since we use \n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        'dogImages/valid',\n",
    "        target_size=(228, 228),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ea3132b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_11 (Conv2D)           (None, 224, 224, 32)      416       \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 224, 224, 32)      4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 224, 224, 32)      128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 112, 112, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 112, 112, 32)      4128      \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 112, 112, 32)      4128      \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 112, 112, 32)      128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 56, 56, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 56, 56, 64)        8256      \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 56, 56, 64)        16448     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 56, 56, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 28, 28, 128)       32896     \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 28, 28, 128)       65664     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 28, 28, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               12845568  \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 133)               34181     \n",
      "=================================================================\n",
      "Total params: 13,148,165\n",
      "Trainable params: 13,147,653\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu', input_shape=(224, 224, 3)))\n",
    "model.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu', input_shape=(224, 224, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(Conv2D(filters=128, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "\n",
    "### TODO: Define your architecture.\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c36091b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6390a1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "418/418 [==============================] - 280s 662ms/step - loss: 5.7014 - accuracy: 0.0082 - val_loss: 4.8843 - val_accuracy: 0.0096\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.88429, saving model to saved_models\\weights.best.augmentation.10epochs.hdf5\n",
      "Epoch 2/3\n",
      "418/418 [==============================] - 128s 306ms/step - loss: 4.9247 - accuracy: 0.0115 - val_loss: 4.8791 - val_accuracy: 0.0108\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.88429 to 4.87908, saving model to saved_models\\weights.best.augmentation.10epochs.hdf5\n",
      "Epoch 3/3\n",
      "418/418 [==============================] - 131s 314ms/step - loss: 4.8943 - accuracy: 0.0111 - val_loss: 4.8751 - val_accuracy: 0.0108\n",
      "\n",
      "Epoch 00003: val_loss improved from 4.87908 to 4.87509, saving model to saved_models\\weights.best.augmentation.10epochs.hdf5\n"
     ]
    }
   ],
   "source": [
    "### TODO: specify the number of epochs that you would like to use to train the model.\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "### Do NOT modify the code below this line.\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.augmentation.10epochs.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "history =  model.fit(train_generator, \n",
    "                  validation_data=validation_generator,\n",
    "                  epochs=epochs,  batch_size=20, callbacks=[checkpointer], verbose=1) \n",
    "# steps_per_epoch=2000 // batch_size,\n",
    "# validation_steps=800 // batch_size,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ffba76c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model.load_weights('saved_models/weights.best.augmentation.10epochs.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33710352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get index of predicted dog breed for each image in test set\n",
    "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\n",
    "print('Test accuracy after 5 epochs: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8e4cb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743b4caf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8202368f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629c029e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
